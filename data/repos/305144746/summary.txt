Tinygrad is a minimalist deep learning framework written in Python that provides a middle ground between PyTorch's complexity and micrograd's simplicity. Targeting machine learning researchers and developers, it implements core neural network functionality including automatic differentiation, tensor operations, and neural network primitives with extreme architectural simplicity. The framework supports multiple hardware accelerators (CUDA, OpenCL, Metal, WebGPU, AMD, LLVM) through a unified interface requiring only ~25 low-level operations per backend. Tinygrad enables both training and inference for production models like LLaMA and Stable Diffusion while maintaining a RISC-like design philosophy that makes adding new accelerators straightforward. It serves ML practitioners who need a lightweight, hackable alternative to heavyweight frameworks without sacrificing essential deep learning capabilities.