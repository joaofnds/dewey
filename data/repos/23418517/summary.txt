Apache Hadoop is a Java-based, distributed processing framework designed for storing and processing large datasets across clusters of commodity hardware. It enables parallel processing of massive amounts of data through its core components: Hadoop Distributed File System (HDFS) for storage and MapReduce for processing. Hadoop facilitates scalable data analysis, data warehousing, and big data applications, allowing users to analyze unstructured and structured data at scale, making it a fundamental framework for big data infrastructure. This project functions as a core framework for big data processing.